[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "The Antichrist or Infinite Games\n\n\nCan we really convince each other of alternate political theories?\n\n\n\n\n\nJul 13, 2025\n\n\nKeith Stevens\n\n\n\n\n\n\n\n\n\n\n\n\nThe AI Tornado\n\n\nLet’s apply a 1990’s tech marketing theory to today’s hot topic: Generative AI Markets\n\n\n\n\n\nDec 11, 2023\n\n\nKeith Stevens\n\n\n\n\n\n\n\n\n\n\n\n\nLocal LLama in Electron JS\n\n\nI too want my own native app with local Llama models and so can you\n\n\n\n\n\nDec 7, 2023\n\n\nKeith Stevens\n\n\n\n\n\n\n\n\n\n\n\n\nKT Villa Overview\n\n\nHow I ended up building a booking website with too much Generative AI\n\n\n\n\n\nNov 30, 2023\n\n\nKeith Stevens\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Keith Stevens",
    "section": "",
    "text": "With 20 years practicing Buddhism and 15 years working in Natural Language Processing, I I’m convinced that language technology works best when it helps people live economically better lives so they can spend more time building and enjoying the human relationships that give life meaning. More so, the best solutions come from teams that focus deeply on a poignant market problem and solve it with relentless focus. And that’s what I hope to keep doing.\nOver 9 years I worked with the Google Translate team while we did exactly that for machine translation. We got to live through the paradigm shift from old school statistical NLP to large transformer models, being one of the first teams in Google to productionize transformer models. With my direct teams we built systems that discovered, curated, and housed the vast pools of data needed to train these models. And with a small team I personally led we build out user facing features that let users share data and insights with us.\nI then left Google and spent about 2 and a half years seeing the startup world up close. I tried replicating some of my Google work as an open source project. Partnered with a co-founder to try making an emotional intelligence oriented AI startup. I’ve since been working with several startups to build out their LLM infused products. Until mid 2024, I plan to continue doing this will splitting my time between Tokyo and Hakuba Japan. In my spare time I’ll also be skiing and writing mini reports on various LLM adjacent open source projects that catch my interest.\nNow tho, I’m back at Google working on Gemini in the Bay Area. That’s included some fun work like ensuring Gemini’s data science like capabilities make life easier for anyone that hates using Excel and work that is soon to released. All this while my wife is doing her MBA at UC Berkeley."
  },
  {
    "objectID": "posts/231209-the-ai-tornado/index.html",
    "href": "posts/231209-the-ai-tornado/index.html",
    "title": "The AI Tornado",
    "section": "",
    "text": "Today’s hot disruptive technology has one name, Generative AI. We know it’s disrupting things and generating tons of investment and tons of money with lots of companies acting hastily. But what should we expect of the market? What is an engineer to make of the upcoming future? I don’t know but during the 90’s a tech marketing master named Geoffrey Moore wrote two books, Crossing the Chasm and Into the Tornado. Together I find them a perfect description of what tech “distruption” really means, and how a disruptive technology flows through markets. Does it still apply to today’s digital SaaS products? I think so. And even if it doesn’t I’m going to fit a square into a circle and see what happens.\n\n\n\n\n\nSo, what is Tornado Theory, as I understand it? This is an older name for what we now call high growth markets, aka the land of startups and aspiring unicorns. In these scenarios, there’s existing or old markets comfortable with some old timey slow and reliable solution. Some new technology shows up and impresses the high tech audience (my crowd) until it gains the attention of some small set of visionary business leaders that see a bit into the future. Then most startups die as they fall into a chasm where they fail to market the product to the much larger and more pragmatic customers that their investors need. The few startups utilizing the new tech that make it through go onto strategic market land grabs until the large pool of pragmatic customers feel ready to buy. Then suddenly every pragmatic customer wants the product and they want it now, they want it fast, and they want it from the market leader. Those that win in the tornado come out unicorns.\nSuccessful companies need to deploy different leadership qualities to survive each stage. At the visionary stage they need to get product innovation right. Just after the chasm, they need to keep developing the product, partner aggressively, and focus on the customer. But as soon as the pragmatic customers decide to start buying in mass, successful companies should, according to Moore’s theory, completely forget the customer and their many particular wants and desires and instead 100% focus on delivering the product as fast as possible. Any lost sale due to lack of supply goes to some other competitor for the lifetime of the market. Winners sell hard and fast with a much lower emphasis, even no emphasis, on hard product innovation.\nAs this tornado is crushing older business models and leaving everyone wondering who will survive and who will be the winner, the market naturally sorts companies into three distinct groups: Gorillas, Chimps, and Monkeys. Why these three groups? Because they give nice analogies to relative strength between each player. Gorillas are, well, the winner of the tornado. They set the standard, they define the product category, they take the largest market share and the largest portion of the profits. The chimps are the competitors with equally good, or even sometimes better, technology but who failed to work the market right. They survive by being the nicer company that still cares about the customer and does some key innovation the Gorilla doesn’t do. The monkeys are everyone else that’s busy trying to provide low cost clones of the top line product. Its a little gross and brutal, but so are the market machines that produce unicorns.\n\n\n\n\n\nNow, this theory worked well in the 90s and 2000s when products were generally hard physical things that required a purchase and support from slower distribution channels like retail stores. These days tho high tech products go through the internet with either self-service SaaS or some convoluted enterprise sales flow. Either way, things move faster. I’m too much of an engineer to exactly know how this changes Moore’s theory but I’m pretty sure it just means some parts of the cycle compress and overlap more than they used to. Either way, I’m pretty sure this Tornado is tearing through the tech world right now in the form of Generative AI. So let’s cut to the chase."
  },
  {
    "objectID": "posts/231209-the-ai-tornado/index.html#the-high-tech-marketing-theory-that-works-the-tornado",
    "href": "posts/231209-the-ai-tornado/index.html#the-high-tech-marketing-theory-that-works-the-tornado",
    "title": "The AI Tornado",
    "section": "",
    "text": "Today’s hot disruptive technology has one name, Generative AI. We know it’s disrupting things and generating tons of investment and tons of money with lots of companies acting hastily. But what should we expect of the market? What is an engineer to make of the upcoming future? I don’t know but during the 90’s a tech marketing master named Geoffrey Moore wrote two books, Crossing the Chasm and Into the Tornado. Together I find them a perfect description of what tech “distruption” really means, and how a disruptive technology flows through markets. Does it still apply to today’s digital SaaS products? I think so. And even if it doesn’t I’m going to fit a square into a circle and see what happens.\n\n\n\n\n\nSo, what is Tornado Theory, as I understand it? This is an older name for what we now call high growth markets, aka the land of startups and aspiring unicorns. In these scenarios, there’s existing or old markets comfortable with some old timey slow and reliable solution. Some new technology shows up and impresses the high tech audience (my crowd) until it gains the attention of some small set of visionary business leaders that see a bit into the future. Then most startups die as they fall into a chasm where they fail to market the product to the much larger and more pragmatic customers that their investors need. The few startups utilizing the new tech that make it through go onto strategic market land grabs until the large pool of pragmatic customers feel ready to buy. Then suddenly every pragmatic customer wants the product and they want it now, they want it fast, and they want it from the market leader. Those that win in the tornado come out unicorns.\nSuccessful companies need to deploy different leadership qualities to survive each stage. At the visionary stage they need to get product innovation right. Just after the chasm, they need to keep developing the product, partner aggressively, and focus on the customer. But as soon as the pragmatic customers decide to start buying in mass, successful companies should, according to Moore’s theory, completely forget the customer and their many particular wants and desires and instead 100% focus on delivering the product as fast as possible. Any lost sale due to lack of supply goes to some other competitor for the lifetime of the market. Winners sell hard and fast with a much lower emphasis, even no emphasis, on hard product innovation.\nAs this tornado is crushing older business models and leaving everyone wondering who will survive and who will be the winner, the market naturally sorts companies into three distinct groups: Gorillas, Chimps, and Monkeys. Why these three groups? Because they give nice analogies to relative strength between each player. Gorillas are, well, the winner of the tornado. They set the standard, they define the product category, they take the largest market share and the largest portion of the profits. The chimps are the competitors with equally good, or even sometimes better, technology but who failed to work the market right. They survive by being the nicer company that still cares about the customer and does some key innovation the Gorilla doesn’t do. The monkeys are everyone else that’s busy trying to provide low cost clones of the top line product. Its a little gross and brutal, but so are the market machines that produce unicorns.\n\n\n\n\n\nNow, this theory worked well in the 90s and 2000s when products were generally hard physical things that required a purchase and support from slower distribution channels like retail stores. These days tho high tech products go through the internet with either self-service SaaS or some convoluted enterprise sales flow. Either way, things move faster. I’m too much of an engineer to exactly know how this changes Moore’s theory but I’m pretty sure it just means some parts of the cycle compress and overlap more than they used to. Either way, I’m pretty sure this Tornado is tearing through the tech world right now in the form of Generative AI. So let’s cut to the chase."
  },
  {
    "objectID": "posts/231209-the-ai-tornado/index.html#aligning-ai-with-our-tornado-market-theory",
    "href": "posts/231209-the-ai-tornado/index.html#aligning-ai-with-our-tornado-market-theory",
    "title": "The AI Tornado",
    "section": "Aligning AI: with our tornado market theory",
    "text": "Aligning AI: with our tornado market theory\nToday’s disruptive technology is most obviously generative AI, in the form of Large Language Models, Multimodal Modals, and Image/Video generation. But what’s not clear is where the Generative AI market is in terms of the standard tech adoption life cycle. Nor is it clear what the true “whole product” solution is. Aside: the whole product is the minimal set of associated products or services needed to maximally leverage the disruptive technology. Also, with the entire space of Generative AI, there’s likely multiple actual markets. So for simplicity, I want to focus on Large Language Models as the baseline disruptive technology.\nThese started pretty early in machine translation where transformers were first applied. Both my old team, Google Translate, and our up and coming competitor, DeepL, demonstrated their visionary behavior. Google Translate partially adopted transformers but also generally favored an alternative technology that turned out to be not super discontinuous. DeepL instead launched fast with transfomers to be the first on the market with a full transformer generative AI product. That was the visionary stage, I think.\n\n\n\n\n\nThen the market wandered through a variety of other niche markets that were larger in size. I wasn’t totally paying attention at this time since I was still with Google Translate, but looking back I’d say companies like Jasper were a key niche market for transformers, now in the form of General Pretrained Transformers or GPT for short. And OpenAI was the leading vendor for transformers. Others were competing and trying things out, but they were building, serving, and hosting the whole core product pretty effectively.\nThen ChatGPT came out and absolutely crushed the bowling alley by gaining 100 million users in light-speed. That week made it absolutely clear that transformers had left the chasm hard. They left the world of niche markets with zero looking back. They had started a tornado. And everyone was buying. A bajillion startups were using OpenAI’s APIs to build thin wrappers that then sold a generative AI product to some niche audience, all while more and more customers consolidated around OpenAI’s product and standards.\n\n\n\n\n\nNow we have a transformative technology, transformers, flowing through the market with clear players. It’s teetering now on main street. What’s not clear is what exactly is the whole product solution that wraps around transformers to make a truly commoditized product any customer can leverage with glee and satisfaction. Nor is it clear who is going to make it through the tornado and end up on top. But let’s make some wild assertions."
  },
  {
    "objectID": "posts/231209-the-ai-tornado/index.html#lets-do-some-positioning-open-ai-and-everyone-else",
    "href": "posts/231209-the-ai-tornado/index.html#lets-do-some-positioning-open-ai-and-everyone-else",
    "title": "The AI Tornado",
    "section": "Let’s do some positioning Open AI and everyone else",
    "text": "Let’s do some positioning Open AI and everyone else\nMoore sets out a few key criteria for placing each company into the Gorilla, Chimp, and Monkey bucket. With Gorilla’s there’s only one. They define the product and the standard that everyone competes with. They get the best profit margin and as they sell, they get stronger. The chimps are solid alternatives that just didn’t quite get main street customer attention as fast. Maybe they sold too slowly for “reasons”. Monkeys are all the companies that sell a product similar to the Gorilla but with key features missing and much lower cost, primarily because they’re not setup to push the market forward.\nWith that reminder, I’m going to throw down some hot takes.\nRight now, OpenAI is clearly the market leader. They are the gorilla. They almost committed suicide with the Board firing Sam Altman, but with them back they’re likely stronger than ever. Why are they the gorilla? Well first, Sam Altman is no dummy. He’s from Y-Combinator and knows how high tech market works and he likely wants to be the winner, even if he doesn’t have an equity stake in the company. More importantly tho, if you look at literally any other LLM product such as LangChain, Pinecone, and any open source product, they integrate with the OpenAI API format. Even brand new open source models find silly ways to fit into OpenAI’s function calling format, however painful it is. I haven’t seen anyone do that for Anthropic’s API, Co:here’s API, and especially not for any of Google’s APIs. So, gorilla is now identified.\n\n\n\n\n\nAnd who are the chimps? The other companies developing and furthering the technology but not taking a market leadership position. Clearly this is Anthropic and Co:here. Anthropic has been pushing forward hard with aligned AI and constitutional AI to carve out their premium positioning. Cohere has been….I’m not quite sure what they’ve been doing but it looks like they’re doing well. Unless Sam Altman gets fired again, neither company will replace OpenAI but they’ll keep going and provide solid meaningful alternatives.\nBut where does Google live? They invented transformers in the first place. They even have a fancy new model called Gemini that they’re about to release. But, they dropped the ball hard in a few ways. Just days ago with Gemini they made what I believe is a hard fatal mistake. They put out a great video that advances customer perception of how they could use generative AI and then they made clear that they did lots of careful behind the scenes editing to make that happen and they didn’t even release the top line model with the video. This means they pushed the market demand forward and expanded the whole product and will be absolutely unable to sell the promised product. They put a delicious lunch on the table and said to every single competitor “Here, eat my lunch. I won’t be doing so”. Whoever actually delivers the product described in the Gemini video will crush it financially, and it won’t be Google. Given Google’s inability to ship, they’re not a gorilla, not a chimp, not even a monkey. They’re a dying gorilla. It won’t kill Google overall, but it puts them out of the game in this new market for sure.\n\n\n\n\n\nNow what about my most controversial statement? That Open Source Models are monkeys? For the consumer market this is maybe not true. Perhaps there’s a key consumer market that needs and wants fully local and private models. And perhaps there’s enterprise customers that absolutely need 100% on premise solutions. Those exist, but they’re not the size of the rest of the market. Open source model developers and hosts will do well in those markets but they without a doubt won’t be unseating OpenAI as a Gorilla. For OpenAI’s key market, they’ll be a low cost and low latency clone. Already Together Compute, which I adore a lot, is selling a whole suite of open source models as a service and marketing it as both faster and cheaper than OpenAI’s services. They don’t describe any key innovations that go beyond cost and speed. That’s clear monkey activity as per Moore. Breaks my heart to say it, but if they act appropriately, they can still make a good deal of money, as long as they don’t get aggressive and pretend to be a Gorilla in OpenAI’s market. Instead it’s for them to develop a side market that OpenAI doesn’t care about and try to position themselves as a market leader."
  },
  {
    "objectID": "posts/231209-the-ai-tornado/index.html#handwaving-towards-the-future",
    "href": "posts/231209-the-ai-tornado/index.html#handwaving-towards-the-future",
    "title": "The AI Tornado",
    "section": "Handwaving towards the future",
    "text": "Handwaving towards the future\nNow, what is the future like? I suspect we’re still figuring out the whole product solution. I’m pretty sure the whole product includes at a minimum:\n\nA Large Language Model, ala GPT-4\nFunction calling, as found in GPT-4\nA semantic vector service like Pinecone\nA retrieval augmented generation (RAG) setup like LangChain\nHosting for all this\n\nFor the last year those were mostly sold separately sold separately with OpenAI, Pinecone, LangChain, and probably Azure or AWS being the key providers. But now with OpenAI’s Assistants API (and the consumer facing GPTs) we can see OpenAI doing exactly what Moore would expect. They are designing out their old partners and commodotized the whole product into a single bundle that OpenAI sells. It may not be the best overall solution, but in majority markets with pragmatic buyers that want a clear singular standard they can rely on, this strategy often works really well and creates the winner. So assuming OpenAI doesn’t lose the ball, they look to be the winner.\n\n\n\n\n\nWhat’s after that? I suspect with AI assistants serving as the commodity whole product, once it gets 100% worked out and crystalized into a standard we’ll see OpenAI start to go back to customer first behaviors as they serve the main market with +1 niche solutions. Already I expect that OpenAI itself wouldn’t even be building out these niches but instead leverage a two sided market place (i.e. the GPTs store). If they do that, then it really is game over for the market with OpenAI being the new Google in the Generative AI market.\nBut, let’s wait and see."
  },
  {
    "objectID": "posts/250713-antichrist-or-infinite-games/index.html",
    "href": "posts/250713-antichrist-or-infinite-games/index.html",
    "title": "The Antichrist or Infinite Games",
    "section": "",
    "text": "For the first time I listened to Interesting Times which, being not a close listener, I thought was hosted by Andrew Ross Sorkin but is in fact hosted by Ross Douthat. Through this first episode I was presented with a rather surprising view into Peter Thiel’s political Philosophy. So much of it was designed for my disagreement, and I’m really glad someone like Ross Douthat was hosting to ask probing questions that forced Thiel to really contend with possible contradictions with his political world view and his every day actions.\nBut alas, what do I find interesting, really, about this podcast? The heart of it is towards the end where we finally get to Thiel’s definition of the Antichrist."
  },
  {
    "objectID": "posts/250713-antichrist-or-infinite-games/index.html#peter-thiel-on-interesting-times",
    "href": "posts/250713-antichrist-or-infinite-games/index.html#peter-thiel-on-interesting-times",
    "title": "The Antichrist or Infinite Games",
    "section": "",
    "text": "For the first time I listened to Interesting Times which, being not a close listener, I thought was hosted by Andrew Ross Sorkin but is in fact hosted by Ross Douthat. Through this first episode I was presented with a rather surprising view into Peter Thiel’s political Philosophy. So much of it was designed for my disagreement, and I’m really glad someone like Ross Douthat was hosting to ask probing questions that forced Thiel to really contend with possible contradictions with his political world view and his every day actions.\nBut alas, what do I find interesting, really, about this podcast? The heart of it is towards the end where we finally get to Thiel’s definition of the Antichrist."
  },
  {
    "objectID": "posts/250713-antichrist-or-infinite-games/index.html#peter-thiels-backing-theory-of-the-world",
    "href": "posts/250713-antichrist-or-infinite-games/index.html#peter-thiels-backing-theory-of-the-world",
    "title": "The Antichrist or Infinite Games",
    "section": "Peter Thiel’s backing theory of the world",
    "text": "Peter Thiel’s backing theory of the world\nSo what is the Antichrist and why is he such a big deal? Well according to Thiel the Antichrist is basically a perfection of everything behind the environmentalist movement, the anti-nuclear movement, and maybe the feminist movement. Its the fundamental cause for why we’ve slowed down “progress” in all the things that could help Thiel live forever. The Antichrist, and his direct or indirect manifestations are why we really don’t have flying cars.\nThat matters because it explains away why society and technology feel stagnant to so many people. Its our desire for “safety” and “security” from nuclear weapons or climate change that has stalled out the world and prevented innovative people from seasteading. But really through his explanation of the Antichrist, I think he also quietly exposed his real primary moral prerogative, the precursor for why the Antichrist is bad.\nHis moral objective is to perfect humanity through technology such that we’re liberated from a dependency upon god. He wants to Übermensch humanities path to be free of god such that we don’t have to depend upon salvation from a second coming of Christ. Humanities future must be in our hands and that means we must live forever and be completely in control of our fate. Its a radical libertarian Christian form of transhumanism come physical.\nAnd that’s why the Antichrist is bad, and any form of slowing down is bad. We must be liberated. We can’t assume God will intervene by delivering a second son to save us from a technocratic safe one world government that makes the future forever the same as now. And that’s why he supports political actors that smash the current taine system.\nSure science is important for progress but Science, the academic institution, is tainted by the Antichrist and his many influences and so its stuck in stagnant thinking and stagnant non-progress. So out it must go. And the current governing bodies are tainted too. They’re infected by DEI, safety, climate mitigation, and all forms of inclusion over progress. So smash it all and maybe we’ll be able to start over from better progress-oriented first principles."
  },
  {
    "objectID": "posts/250713-antichrist-or-infinite-games/index.html#lol-really",
    "href": "posts/250713-antichrist-or-infinite-games/index.html#lol-really",
    "title": "The Antichrist or Infinite Games",
    "section": "LOL, really",
    "text": "LOL, really\nI want to react to this. I want to debate all the particulars about this world view and why it seems….completely insane. And in a very polite way Ross Douthat probed inconsistencies, like why invest in technologies that empower the Antichrist?\nBut then I had a realization, its more informative to actually discuss with myself, and others who might listen, why I react. What is my underlying world philosophy, or moral philosophy, that backs all the disagreements I might have. If i were to debate this over brunch with someone that agreed with Thiel, would we be able to convince each other of anything or would we really just be exposing our “priors”?\nSo rather than go into all that granular nonsense, what’s the prior?"
  },
  {
    "objectID": "posts/250713-antichrist-or-infinite-games/index.html#an-infinite-games-critique-of-our-stagnation",
    "href": "posts/250713-antichrist-or-infinite-games/index.html#an-infinite-games-critique-of-our-stagnation",
    "title": "The Antichrist or Infinite Games",
    "section": "An Infinite Games critique of our stagnation",
    "text": "An Infinite Games critique of our stagnation\nIt sure feels like we’re stagnating. People aren’t getting richer. We don’t have flying cars. We’re not living on the moon. This isn’t what Back to the Future sold me, although it did promise a Biff Tannen president. But are we stagnating?\nEven Thiel admits that we’re actively innovating rather violently in the field of Artificial Intelligence. So maybe there’s a much simpler theory: we are advancing and we always have been, but we only ever advance along dimensions of dominance. Both individuals and societies are stuck playing Finite rather than Infinite games where we aim to demonstrate clear competence, or dominance against our competitors.\nThis fits with critiques others have shared, like Tim Urban. Large groups want to show that they are better than others. Its geopolitical hegemony. Its the 1960’s space race where we want to prove that we (United States) have better rockets than the Soviets. Its the 1940’s where the United States has to ensure everyone knows they have the best bombs. Its right now where the United States gets the best AI systems first.\nWe progress, rather violently, along dimensions where the leading powers are insecure. Insecure about their position or ranking in the finite game of power. Other dimensions of society? Those come second and they just advance along the base rate of improvement because there’s no need to over-invest and dominate. There’s no grand vision to transubstantiate and be free of God, even if people promote progress as such. Its just a violent game of being the first past the finish line.\nAnd we could be better, we could focusing on ensuring the game goes on"
  },
  {
    "objectID": "posts/250713-antichrist-or-infinite-games/index.html#why-safety-isnt-the-antichrist",
    "href": "posts/250713-antichrist-or-infinite-games/index.html#why-safety-isnt-the-antichrist",
    "title": "The Antichrist or Infinite Games",
    "section": "Why safety isn’t the Antichrist",
    "text": "Why safety isn’t the Antichrist\nWhat’s a steel man for the various forms of safety movements? Something to showcase how they aren’t the Antichrist but actually some form of moral goodness. Well, they’re simply trying to ensure that everyone gets to continue playing the Infinite game. If the world blows up because we set off all the nuclear weapons, then well…the game is kind of over for everyone and no transhumanism. If we create a bio-weapon that kills 90% of the human population…again…likely no transhumanism and no going to mars. If the environment goes backwards and kills huge swathes of humanity and triggers constant resource wars, investing in space travel is going to be rather challenging. If we create AI systems that distance humans from each other, there’s no room for moral kindness or sociological stimulation. Humanity ends in a dead end.\nSo maybe the safety movement is really an attempt to ensure the game goes on for as many people as possible. Its asserting that reality is not just a game oriented towards fulfilling Thiel’s ambition to liberate himself from the woes of death but is instead a game for us all to play. To be kind, to be fair, to go on and do something creative. Safety isn’t the Antichrist, its reminder that this is all the only game we have to play."
  },
  {
    "objectID": "posts/231206-local-llama-electron/index.html",
    "href": "posts/231206-local-llama-electron/index.html",
    "title": "Local LLama in Electron JS",
    "section": "",
    "text": "Recently I had to make a trip to China with my wife. These days, that means I’ll be disconnected from the regular internets and totally isolated from the ChatGPTs of the world or even my Linux server hosted open source LLMs. I can get by without these things when it comes to writing emails or writing code, but I do need my LLMs to explore the world of bad dad jokes and get pet care advice. How am I to survive? local models with internet free apps to the rescue!\n\n\n\n\n\nRight now, probably the best and easiest to use app for all this is LM Studio. They let you install a local native app and then download quantized (e.g. compressed) open source models that you run totally locally if you have a sufficiently good computer. I have a Mac M2 with 20 something gigs of ram so I can run any 7B model without question. That lets me get all the dad jokes I need to be productive. But what if LM studio didn’t exist? What would it take to make it? Or what if I felt like I needed something like LM Studio but with a few twists and turns? How hard is it to do?\n\n\n\n\n\nIf you take a peek at what LM Studio is doing in the background, it’s pretty obvious it’s an Electron JS app. This has been a pretty standard way to write a cross platform native app using pure Node.JS. You write up your little server like background process and then a bunch of client side HTML + javascript and with a little bit of compilation you get your own LM Studio, Slack, or Discord app. So, what did LM Studio do to get their app? Let’s dive in and find out."
  },
  {
    "objectID": "posts/231206-local-llama-electron/index.html#why-local-models-why-local-apps",
    "href": "posts/231206-local-llama-electron/index.html#why-local-models-why-local-apps",
    "title": "Local LLama in Electron JS",
    "section": "",
    "text": "Recently I had to make a trip to China with my wife. These days, that means I’ll be disconnected from the regular internets and totally isolated from the ChatGPTs of the world or even my Linux server hosted open source LLMs. I can get by without these things when it comes to writing emails or writing code, but I do need my LLMs to explore the world of bad dad jokes and get pet care advice. How am I to survive? local models with internet free apps to the rescue!\n\n\n\n\n\nRight now, probably the best and easiest to use app for all this is LM Studio. They let you install a local native app and then download quantized (e.g. compressed) open source models that you run totally locally if you have a sufficiently good computer. I have a Mac M2 with 20 something gigs of ram so I can run any 7B model without question. That lets me get all the dad jokes I need to be productive. But what if LM studio didn’t exist? What would it take to make it? Or what if I felt like I needed something like LM Studio but with a few twists and turns? How hard is it to do?\n\n\n\n\n\nIf you take a peek at what LM Studio is doing in the background, it’s pretty obvious it’s an Electron JS app. This has been a pretty standard way to write a cross platform native app using pure Node.JS. You write up your little server like background process and then a bunch of client side HTML + javascript and with a little bit of compilation you get your own LM Studio, Slack, or Discord app. So, what did LM Studio do to get their app? Let’s dive in and find out."
  },
  {
    "objectID": "posts/231206-local-llama-electron/index.html#your-own-electron-js-app-with-local-models",
    "href": "posts/231206-local-llama-electron/index.html#your-own-electron-js-app-with-local-models",
    "title": "Local LLama in Electron JS",
    "section": "Your own Electron JS app with local models",
    "text": "Your own Electron JS app with local models\n\n\n\n\n\nOur end goal is a local app running a model 100% on our local machine and ideally packaged in a single ElectronJS app without any other services. There’s lots of other (easier) ways to crack this nut but let’s aim for this goal because we’re pretending we don’t like accessing apps through our web browsers. So what do you need? Ultimately you need a way to run the quantized LLMs with your bare model CPU or Apple silicon. You could write that yourself or you could leverage llama.cpp like a reasonable person.\nBut llama.cpp is in C++, which last I checked is not javascript, nor typescript. Thankfully @withcatai has solved this problem for us, mostly, by writing node-llama-cpp. This builds llama.cpp as some linkable libraries and adds Javascript bindings so any (almost) Node.JS app can call local models directly form javascript land.\nTo get this working, let’s solve two key requirements:\n\nWe must use ESM modules. Javascript is notorious for having many flavors and none of them work well together. node-llama-cpp chose ESM modules so that’s what we have to pick.\nWe like to be lazy so let’s do the client side in ReactJS. That will introduce some additional challenges.\n\nI’ve done all this already for you with an app I call local-llama-electron, a very creative name. If you want to read the code for yourself, take a minute and come back. Or just fork it and move along without reading below, but you might miss a funny image or two.\nLet’s look at the hardest parts now. Going forward, I’m going to assume you’ve created a vanilla Electron JS app using Electron Forge or you’re reading my repository.\nFirst, ElectronJS doesn’t yet fully support ESM moduels, a hard requirement for node-llama-cpp, but in their upcoming version 28 release they will be. That gets us pretty far. We just need to install the beta releaes and make a few changes to our Electron App after creating it.\nnpm install --upgrade electron@beta\nThe other small change you likely need to do is make sure all the Electron config files are written as ESM modules. This should look like\nexport default {\n  buildIdentifier: \"esm\",\n  defaultResolved: true,\n  packagerConfig: {},\n  rebuildConfig: {},\n  ...\n}\nIf we wanted to write all the client side in bare bones HTML, CSS, and Javascript, we’d be done. But people like me, we want ReactJS and that means we need a tool like WebPack or Vite to bundle client side code into something sensible. Normally Vite handles ESM really well but Electron’s Vite plugin does not. So I forked it to make a one line change that treats everything as ESM instead of some other option. You can find that here.\nYou can install that with something like\nnpm install --upgrade \\\n  \"https://github.com/fozziethebeat/electron-forge-plugin-vite-esm#plugin-vite-esm\" \\\n  --include=dev \\\n  --save\nAre we done yet? Assuming we’ve followed the Electron Forge documentation on setting up ReactJS and Vite? Nope, because Vite does so much work for us, it now complicates node-llama-cpp in one tiny way. It tries to bundle the entire package up for us but manages to leave out the C++ resource libraries.\nI bet there’s a better way to fix this but I edited my vite.main.config.ts file to include this stanza:\nexport default defineConfig({\n  build: {\n    rollupOptions: {\n      external: [\n        \"node-llama-cpp\",\n      ],\n    },\n  },\n  ...\n});\nNow we’ve got a fully functioning independent Local Llama Electron App. But let’s go further and test the limits of what we could build with some additional work."
  },
  {
    "objectID": "posts/231206-local-llama-electron/index.html#breaking-out-of-the-electron-box",
    "href": "posts/231206-local-llama-electron/index.html#breaking-out-of-the-electron-box",
    "title": "Local LLama in Electron JS",
    "section": "Breaking out of the Electron Box",
    "text": "Breaking out of the Electron Box\nSo far we set ourselves a goal and we hit it hard. We wanted a single app we can package and distribute that lets us run local models as a native app and that we got. But right now there’s a few limitations to what node-llama-cpp can do:\n\nIt doesn’t support Multimodal models like Llava even though llama.cpp does.\nIt doesn’t support streaming (again even though llama.cpp does).\nTo my knowledge, no one has built a llama.cpp for SDXL Turbo, the latest fast version of Stable Diffusion that you can run locally with a python setup.\n\nSo let’s expand these shennanigans with these working bits just to see if it feels fun and useful. Later we can figure out how to get everything back into the single Electron box.\nAt the end of the day, we’ll end up with something absurd like this:\n\n\n\n\n\nflowchart LR\n  Client[React JS]\n  Server[Main]\n  NodeLlama(node-llama-cpp)\n  LocalLlamaPython[local-llama-python]\n  LocalSDXLTurbo[local-sdxl-turbo]\n  SDXLTurbo(sdxl-turbo)\n  Llava(llava-1.5-7b)\n\n  subgraph ElectronJS\n    Client --&gt; Server\n    Server --&gt; NodeLlama\n  end\n\n  subgraph Python Server 1\n    Server --&gt; LocalLlamaPython\n    LocalLlamaPython --&gt; Llava\n  end\n\n  subgraph Python Server 2\n    Server --&gt; LocalSDXLTurbo\n    LocalSDXLTurbo --&gt; SDXLTurbo\n  end\n\n\n\n\n\n\n\nPutting the Llava in the multimodal\nAs stated, llama.cpp already supports running Multimodal modals like Llava 1.5 7B. This is pretty rad because it lets us take an image, run it through an embedding step and then feed that into a standard LLM to get some text description. We can even add arbitrary prompting related to the image. To fancy up our prototype, we can use llama-cpp-python, which is very much like node-llama-cpp but done for Python. Not only does this support Llava models, it also provides an OpenAI compatible server supporting the vision features.\nThat means, just go on over to llama-cpp-python, install it, download your favorite multimodal modal and turn it on! For me that looked like\npython -m llama_cpp.server \\\n  --model ~/.cache/lm-studio/models/mys/ggml_llava-v1.5-7b/ggml-model-q5_k.gguf \\\n  --model_alias llava-1.5 \\\n  --clip_model_path ~/.cache/lm-studio/models/mys/ggml_llava-v1.5-7b/mmproj-model-f16.gguf \\\n  --chat_format llava-1-5 \\\n  --n_gpu_layers 1\nNOTE: One big caveat. If you’re running on MacOS with an M2 chip, you might have an impossible time installing version 0.2.20. I added my solution to this issue, maybe it’ll help you too.\nWith that setup in a separate process, we just need to do our very standard app building and call the new fake OpenAI endpoint in our main process:\nasync function analyzeImage(event) {\n  // Get yo images.\n  const { filePaths } = await dialog.showOpenDialog({\n    filters: [{ name: \"Images\", extensions: [\"jpg\", \"jpeg\", \"png\", \"webp\"] }],\n    properties: [\"openFile\"],\n  });\n  // Tell the client side that we got the file and give it our local protocol\n  // that's handled properly for electron.\n  event.reply(\"image-analyze-selection\", `app://${filePaths[0]}`);\n  // Later, this should actually call a node-llama-cpp model.  For now we call\n  // llama-cpp-python through the OpenAI api.\n  const result = await mlmOpenai.chat.completions.create({\n    model: \"llava-1.5\",\n    messages: [\n      {\n        role: \"user\",\n        content: [\n          { type: \"text\", text: \"What’s in this image?\" },\n          {\n            type: \"image_url\",\n            image_url: `file://${filePaths[0]}`,\n          },\n        ],\n      },\n    ],\n    stream: true,\n  });\n  // Get each returned chunk and return it via the reply callback.  Ideally\n  // there should be a request ID so the client can validate each chunk.\n  for await (const chunk of result) {\n    const content = chunk.choices[0].delta.content;\n    if (content) {\n      event.reply(\"image-analyze-reply\", {\n        content,\n        done: false,\n      });\n    }\n  }\n  // Let the callback know that we're done.\n  event.reply(\"image-analyze-reply\", {\n    content: \"\",\n    done: true,\n  });\n}\nNow we can let a user click a button, select and image, and get some fancy text like below.\n\n\n\n\n\n\n\nNow let’s do it with images\nWe’re not satisfied with just generative text, nor with image to text models. No. No. No. We want the whole enchilada. We want text to image generation running all locally so we can get ourselves a full fledged all modality generative AI app.\nSo let’s drop the whole C++ requirement and write a tiny little OpenAI compatible API server that hosts SDXL Turbo. With a proper python setup this is pretty easy and we can again call that server fro, our Electron app with a REST API call.\nI did that for you, even tho it’s pretty easy. It’s over at local-sdxl-turbo. Download it, install, and run. Running is as simple as\npython -m server --device mps\nThen you too can add this tiny bit of Javascript to get generative images in your Electron App:\nasync function generateImage(event, prompt) {\n  // Later, this should actually call a node-llama-cpp model.  For now we call\n  // llama-cpp-python through the OpenAI api.\n  const result = await imageOpenai.images.generate({\n    prompt,\n    model: \"sdxl-turbo\",\n  });\n  return result.data[0].b64_json;\n}\nAnd with your image prompting skills, you too can try to replicate high tech marketing theories as artistic masterpieces."
  },
  {
    "objectID": "posts/231206-local-llama-electron/index.html#recap",
    "href": "posts/231206-local-llama-electron/index.html#recap",
    "title": "Local LLama in Electron JS",
    "section": "Recap",
    "text": "Recap\nMy little Local Llama Electron app is by no means meant to be a real usable product. It’s janky. It’s kinda ugly (although I do like the DaisyUI cupcake color palette). It’s also hard to setup and deploy.\nBut it is a demonstration of what’s possible these days with local models. With a bit more extra work you can have a fully packaged system. To get there you just need to:\n\nReplicate some of the work done by node-llama-cpp to include support for multi-modal modals.\nDo the whole llama.cpp thing but for SDXL-turbo. I’m sure someone has done it and I just don’t know. If so, then you just need the javascript bindings.\n\nAnd then you have a pretty fancy pants multi-modal LLM app for anyone to use.\nI might get around to doing those extra steps and documenting them, but no promises. It turns out even writing this blog post while in China is a massive pain."
  },
  {
    "objectID": "posts/kt-villa-overview/index.html",
    "href": "posts/kt-villa-overview/index.html",
    "title": "KT Villa Overview",
    "section": "",
    "text": "KT Villa is a nifty little booking website I hand built to manage trips to my winter lodge in Hakuba Japan. For entirely silly reasons I infused it with way more Generative AI than needed. Why? Well it all starts with little Meiji Chocolates in Japan where each tube comes with some delightful little sticker representing a country somewhere in the world. I’ve been collecting these over the years and thought it would be fun to do the same thing with bar soap, each purchase comes with a little trading card that has an AI generated image and an AI generated character profile. Like NFTs, people could “claim” them on a website but unlike NFTs they’re intentionally worthless. Seems pretty doable but it requires a soap company selling real products, which I aspire to have one day but currently do not (message me if you’re keen to do something silly like this plz).\n\n\n\n\n\nI do however have an empty winter lodge in Hakuba Japan, a prime place to enjoy the snow. And I have many friends who wish to stay there on the regular. So instead of generated images and character cards for soap purchases, I did the same thing for trips to my winter lodge. And bam, that’s how we get KT Villa. As a fun aside, the Villa part comes from a previous name for our house, Chill Villa, when it was on AirBnB and the KT part comes from nicknames for my wife and myself, Koala and Tree. Cute right?\n\nPretty much all the hard part of the site is just your regular old booking website challenges. Managing users, managing booking details, making sure two trips don’t overlap, making sure timezones are all done right. Making it look visually okay. It’s an entirely normal website except when you make a booking, you get a SDXL (Stable Diffusion XL from Stability AI) generated image. To boot, I setup SDXL to run a different themed LoRA adapter every two weeks so with a bit of prompt magic and adapter swapping, every trip will generate a fairly unique image.\n\nBut I wanted something even sillier. Since I’ve been contracting with a team focusing on themed chatbots, I wanted to take it to the extreme. For every generated imaged, I let users create a character profile card using a multi-modal LLM. That gives us a pretty unpredictable chatbot profile and when paired with a decent small chat trained LLM, users can try chatting with their booking image and see what silliness entails. With time I also plan to somehow cram in Retrieval Augmented Generation somewhere using LangChain.\n\nI honestly doubt any of this is really useful, but it felt like a fun way to try blending together real services and products with a little bit of AI flair, predominantly for the purpose of zaney entertainment. And with the state of things today, it was pretty easy to do even with entirely self hosted models. Let’s find out just how easy."
  },
  {
    "objectID": "posts/kt-villa-overview/index.html#the-origins-for-kt-villa",
    "href": "posts/kt-villa-overview/index.html#the-origins-for-kt-villa",
    "title": "KT Villa Overview",
    "section": "",
    "text": "KT Villa is a nifty little booking website I hand built to manage trips to my winter lodge in Hakuba Japan. For entirely silly reasons I infused it with way more Generative AI than needed. Why? Well it all starts with little Meiji Chocolates in Japan where each tube comes with some delightful little sticker representing a country somewhere in the world. I’ve been collecting these over the years and thought it would be fun to do the same thing with bar soap, each purchase comes with a little trading card that has an AI generated image and an AI generated character profile. Like NFTs, people could “claim” them on a website but unlike NFTs they’re intentionally worthless. Seems pretty doable but it requires a soap company selling real products, which I aspire to have one day but currently do not (message me if you’re keen to do something silly like this plz).\n\n\n\n\n\nI do however have an empty winter lodge in Hakuba Japan, a prime place to enjoy the snow. And I have many friends who wish to stay there on the regular. So instead of generated images and character cards for soap purchases, I did the same thing for trips to my winter lodge. And bam, that’s how we get KT Villa. As a fun aside, the Villa part comes from a previous name for our house, Chill Villa, when it was on AirBnB and the KT part comes from nicknames for my wife and myself, Koala and Tree. Cute right?\n\nPretty much all the hard part of the site is just your regular old booking website challenges. Managing users, managing booking details, making sure two trips don’t overlap, making sure timezones are all done right. Making it look visually okay. It’s an entirely normal website except when you make a booking, you get a SDXL (Stable Diffusion XL from Stability AI) generated image. To boot, I setup SDXL to run a different themed LoRA adapter every two weeks so with a bit of prompt magic and adapter swapping, every trip will generate a fairly unique image.\n\nBut I wanted something even sillier. Since I’ve been contracting with a team focusing on themed chatbots, I wanted to take it to the extreme. For every generated imaged, I let users create a character profile card using a multi-modal LLM. That gives us a pretty unpredictable chatbot profile and when paired with a decent small chat trained LLM, users can try chatting with their booking image and see what silliness entails. With time I also plan to somehow cram in Retrieval Augmented Generation somewhere using LangChain.\n\nI honestly doubt any of this is really useful, but it felt like a fun way to try blending together real services and products with a little bit of AI flair, predominantly for the purpose of zaney entertainment. And with the state of things today, it was pretty easy to do even with entirely self hosted models. Let’s find out just how easy."
  },
  {
    "objectID": "posts/kt-villa-overview/index.html#the-tech-stack",
    "href": "posts/kt-villa-overview/index.html#the-tech-stack",
    "title": "KT Villa Overview",
    "section": "The Tech Stack",
    "text": "The Tech Stack\n\n\n\n\n\nflowchart LR\n  React[React JS]\n  Fastify[Fastify Server]\n  Postgres[Postgres Database]\n  SurfaceChat[Surface Chat FastAPI]\n  LLaVa(LLaVa LLM)\n  SDXL(SDXL + LoRAs)\n  Zephyr(Zephyr 7B Beta)\n  FastChat[FastChat FastAPI]\n\n  subgraph RedwoodJS\n    React --&gt; Fastify\n    Fastify --&gt; Postgres\n  end\n\n  subgraph Custom FastAPI Python Server \n    Fastify --&gt; SurfaceChat\n    SurfaceChat --&gt; LLaVa\n    SurfaceChat --&gt; SDXL\n  end\n\n  subgraph Standard FastChat Python Server\n    Fastify --&gt; FastChat\n    FastChat --&gt; Zephyr\n  end\n\n\n\n\n\n\nWhen building this, I wanted to keep to a fairly standard tech stack that let me easily separate out potentially re-usable parts from all the booking-specific parts. I also wanted to leverage tools I was already pretty familiar with since this is mostly to simplify managing my winter lodge and only partially for fun. That led to a few key decisions, RedwoodJS for the primary client side experience, Postgres for all database stuff, FastAPI for any custom LLM servers, and ready to go server like FastChat for whatever could fit in it.\nThat’s how we got to this horrible monstrosity with three docker images and way too many containers running on my single NVIDIA RTX A6000 powered server. But with the absurd fleet of containers and some simple docker compose scripts, its actually pretty simple.\nIf you haven’t tried it yet, RedwoodJS is a wrapper around a ReactJS client side experience and a Fastify backend Node.JS server. It pairs well with a database like Postgres and does a bunch of GraphQL magic that I don’t like thinking about. I find it a lot easier to get started than something like Next.JS since Redwood doesn’t have any weird blending between client side and server side behavior and just templates out the annoying parts. If you like something straight forward, I very much recommend RedwoodJS. And pair it with Daisy UI for styling everything and you can pretty quickly get a nice looking hand crafted website.\nFor all the image related components, both generating images and turning images into character profiles, I hacked together a very lightweight FastAPI python server that I call Surface Chat. Its pretty much me glueing together various transformers API tutorials so that I can run a baseline SDXL model and swap in a bunch of different LoRA adapters on demand. Crammed in is LLaVA for multi-modal models mostly because I didn’t know of any open source framework that supported this with a standard format, but with OpenAI’s releases that will likely change soon.\nFinally, for the chat portion and other random places I might need pure a generative text model, I spun up FastChat with a single Zephyr 7B Beta model. FastChat has their own suite of tools for training but I primarily wanted their nice split framework so I can scale the heavy GPU portion easily and it’s support for various serving methods. And there’s probably newer options for this step but I haven’t investigated them yet. That is somewhere in my future."
  },
  {
    "objectID": "posts/kt-villa-overview/index.html#what-next",
    "href": "posts/kt-villa-overview/index.html#what-next",
    "title": "KT Villa Overview",
    "section": "What Next",
    "text": "What Next\nWhew that was a lot. This was really a whirlwind summary of what KT Villa is all about, why I made it, and a light sketch of what went into it. Up next I want to go into more detail about what’s happening inside SurfaceChat and generally what I like and don’t like about FastChat. Lastly I’ll go in depth through the RedwoodJS server and show where all the generative AI steps tie in and demonstrate how straightforward it can be with this kind of separation. Expect those in coming weeks.\nP.S. If you want to make a cute website like this, take @HamelHusain’s advice and look at Quarto. I used to use Jekyll and this is so so much better."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Open Source Musings & Snow Time",
    "section": "",
    "text": "Welcome! Here’s where I write thoughts on various open source Large Language Model projects and random things I’m doing. I’m a former Staff Software Engineer from Google Translate and until I make it back the states, I’m spending some time exploring the world of smaller teams and companies. I sometimes take on contractor work related to LLMs. During winter, I spend a lot of time skiing in Hakuba Japan (if you find yourself there, say hello and I can show you around)."
  },
  {
    "objectID": "index.html#recent-posts",
    "href": "index.html#recent-posts",
    "title": "Open Source Musings & Snow Time",
    "section": "Recent Posts",
    "text": "Recent Posts\n\n\n\n\n\n\n\n\n\n\nThe Antichrist or Infinite Games\n\n\nCan we really convince each other of alternate political theories?\n\n\n\n\n\nJul 13, 2025\n\n\nKeith Stevens\n\n\n\n\n\n\n\n\n\n\n\n\nThe AI Tornado\n\n\nLet’s apply a 1990’s tech marketing theory to today’s hot topic: Generative AI Markets\n\n\n\n\n\nDec 11, 2023\n\n\nKeith Stevens\n\n\n\n\n\n\n\n\n\n\n\n\nLocal LLama in Electron JS\n\n\nI too want my own native app with local Llama models and so can you\n\n\n\n\n\nDec 7, 2023\n\n\nKeith Stevens\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "random.html",
    "href": "random.html",
    "title": "Random things",
    "section": "",
    "text": "I took Mandarin Chinese back in 2010 while in grad school and it was really helpful with my buddhist practice and when I ended up visiting China. Now tho I need to improve my mandarin due to my Chinese family in law. So, time to find a decent class. Here’s a few things I’m looking at.\n\nGo East Mandarin Maybe a good set of group lessons? Group seems nice to put the social pressure on me."
  },
  {
    "objectID": "random.html#taking-mandarin-classes",
    "href": "random.html#taking-mandarin-classes",
    "title": "Random things",
    "section": "",
    "text": "I took Mandarin Chinese back in 2010 while in grad school and it was really helpful with my buddhist practice and when I ended up visiting China. Now tho I need to improve my mandarin due to my Chinese family in law. So, time to find a decent class. Here’s a few things I’m looking at.\n\nGo East Mandarin Maybe a good set of group lessons? Group seems nice to put the social pressure on me."
  }
]